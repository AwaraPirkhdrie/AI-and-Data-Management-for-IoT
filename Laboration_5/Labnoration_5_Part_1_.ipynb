{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8c6da3",
   "metadata": {},
   "source": [
    "# Project 1: Machine Learning Project without Mllib Pipline\n",
    "Name: Awara Pirkhdrie, \n",
    "Date: 2024-02-29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ccea06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.5.1-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0f05d",
   "metadata": {},
   "source": [
    "# Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38fda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28669f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Titanic Data\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "919ea6b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-O8EUHK6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Titanic Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1dd3d3b90d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c467e4",
   "metadata": {},
   "source": [
    "# Reading Data\n",
    "• Dataset (Titanic): https://www.kaggle.com/c/titanic/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "748d89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates DataFrame 'df' by reading a CSV file with Spark, including column names and specifying the file path.\n",
    "df = (spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"./Dataset/titanic/train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "147b64b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60a38d",
   "metadata": {},
   "source": [
    "# Selecting some columns (if needed)\n",
    "• From pyspark.sql.functions import col and then select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84d2054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2bc6e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects and type transforms columns from DataFrame for analysis.\n",
    "dataset = df.select(col(\"Survived\").cast(\"float\"),\n",
    "                    col(\"Pclass\").cast(\"float\"),\n",
    "                    col(\"Sex\"),\n",
    "                    col(\"Age\").cast(\"float\"),\n",
    "                    col(\"Fare\").cast(\"float\"),\n",
    "                    col(\"Embarked\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "171c106d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-------+--------+\n",
      "|Survived|Pclass|   Sex| Age|   Fare|Embarked|\n",
      "+--------+------+------+----+-------+--------+\n",
      "|     0.0|   3.0|  male|22.0|   7.25|       S|\n",
      "|     1.0|   1.0|female|38.0|71.2833|       C|\n",
      "|     1.0|   3.0|female|26.0|  7.925|       S|\n",
      "|     1.0|   1.0|female|35.0|   53.1|       S|\n",
      "+--------+------+------+----+-------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a6251",
   "metadata": {},
   "source": [
    "# Removing null values (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f851634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1bbf3bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---+----+--------+\n",
      "|Survived|Pclass|Sex|Age|Fare|Embarked|\n",
      "+--------+------+---+---+----+--------+\n",
      "|       0|     0|  0|177|   0|       2|\n",
      "+--------+------+---+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counts and displays the number of null values per column in the dataset.\n",
    "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e0bb21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replaces \"?\" with None and removes rows with any None.\n",
    "dataset = dataset.replace(\"?\", None).dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8a31a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---+----+--------+\n",
      "|Survived|Pclass|Sex|Age|Fare|Embarked|\n",
      "+--------+------+---+---+----+--------+\n",
      "|       0|     0|  0|  0|   0|       0|\n",
      "+--------+------+---+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Counts null values in each column and displays the result.\n",
    "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348c6f7",
   "metadata": {},
   "source": [
    "# Converting categorical variables to numeric values\n",
    "• Spark only supports numeric values and is incapable of handling categorical variables. For modeling, all categorical\n",
    "variables must be converted to numeric values. To achieve this, StringIndexer is employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0b49e314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-------+--------+\n",
      "|Survived|Pclass|   Sex| Age|   Fare|Embarked|\n",
      "+--------+------+------+----+-------+--------+\n",
      "|     0.0|   3.0|  male|22.0|   7.25|       S|\n",
      "|     1.0|   1.0|female|38.0|71.2833|       C|\n",
      "|     1.0|   3.0|female|26.0|  7.925|       S|\n",
      "+--------+------+------+----+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "afa3b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "86c49f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts text column \"Sex\" to numeric values in new column \"Gender\".\n",
    "dataset = StringIndexer(\n",
    "    inputCol=\"Sex\", \n",
    "    outputCol=\"Gender\", \n",
    "    handleInvalid=\"keep\").fit(dataset).transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fe431bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes text column \"Embarked\" to numeric \"Boarded\", handling invalids with \"keep\".\n",
    "dataset = StringIndexer(\n",
    "    inputCol=\"Embarked\", \n",
    "    outputCol=\"Boarded\", \n",
    "    handleInvalid=\"keep\").fit(dataset).transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4af307f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-------+--------+------+-------+\n",
      "|Survived|Pclass|   Sex| Age|   Fare|Embarked|Gender|Boarded|\n",
      "+--------+------+------+----+-------+--------+------+-------+\n",
      "|     0.0|   3.0|  male|22.0|   7.25|       S|   0.0|    0.0|\n",
      "|     1.0|   1.0|female|38.0|71.2833|       C|   1.0|    1.0|\n",
      "+--------+------+------+----+-------+--------+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f454fec0",
   "metadata": {},
   "source": [
    "##### Finally, we can drop ‘sex’ and ‘ embarked’ columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2510f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes the 'Sex' and 'Embarked' columns from the dataset sequentially.\n",
    "# Drop unnecessary columns\n",
    "dataset = dataset.drop('Sex')\n",
    "dataset = dataset.drop('Embarked')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb8dab7",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "• Spark learns through two columns, label and feature. Therefore, all columns except the target column must be combined\n",
    "into a single column. This is accomplished via VesctorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a47cecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler as va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cb2acfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregates specific columns into a 'features' column for machine learning models.\n",
    "require_featured = ['Pclass', 'Age', 'Fare', 'Gender', 'Boarded']\n",
    "assembler = va(inputCols=require_featured, outputCol='features') \n",
    "transformed_data = assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "90e4a0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-------+------+-------+--------------------+\n",
      "|Survived|Pclass| Age|   Fare|Gender|Boarded|            features|\n",
      "+--------+------+----+-------+------+-------+--------------------+\n",
      "|     0.0|   3.0|22.0|   7.25|   0.0|    0.0|[3.0,22.0,7.25,0....|\n",
      "|     1.0|   1.0|38.0|71.2833|   1.0|    1.0|[1.0,38.0,71.2833...|\n",
      "|     1.0|   3.0|26.0|  7.925|   1.0|    0.0|[3.0,26.0,7.92500...|\n",
      "|     1.0|   1.0|35.0|   53.1|   1.0|    0.0|[1.0,35.0,53.0999...|\n",
      "|     0.0|   3.0|35.0|   8.05|   0.0|    0.0|[3.0,35.0,8.05000...|\n",
      "+--------+------+----+-------+------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aaf897",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "29767cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 583\n",
      "Number of test samples: 129\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "First the transformed dataset into training and test sets with a distribution of 80% for training and 20% for testing. \n",
    "Then a RandomForest classifier is created with the specification of the target column ('Survived') and the \n",
    "feature column ('features'). The model is trained with the training data and used to make predictions on the test data, \n",
    "resulting in a new DataFrame with the predictions.\n",
    "'''\n",
    "\n",
    "# Splitting the dataset into train and test\n",
    "(training_data, testing_data) = transformed_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "print(\"Number of train samples: \" + str(training_data.count()))\n",
    "print(\"Number of test samples: \" + str(testing_data.count()))\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='Survived', \n",
    "                            featuresCol='features', \n",
    "                            maxDepth=5)\n",
    "\n",
    "model = rf.fit(training_data)\n",
    "predictions = model.transform(testing_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e507adb",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "38e80d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.7596899224806202\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Multi-class classification evaluator using PySpark's ML library, set to evaluate the accuracy ('accuracy') \n",
    "by comparing the actual survival results ('Survived') with the predicted ones ('prediction') from the model. \n",
    "The accuracy of the model's predictions on the test data is calculated and then printed.\n",
    "'''\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Survived', \n",
    "                                              predictionCol='prediction', \n",
    "                                              metricName='accuracy')\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Training Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f06a3",
   "metadata": {},
   "source": [
    "# Checking Spark jobs\n",
    "After all you can check Spark Jobs on your local machine and manage them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb1721b",
   "metadata": {},
   "source": [
    "![](images\\2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64670e96",
   "metadata": {},
   "source": [
    "![](images\\3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f31dd72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
