{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92880fe6",
   "metadata": {},
   "source": [
    "# Project 2: Machine Learning Project with Mllib Pipline\n",
    "Name: Awara Pirkhdrie, \n",
    "Date: 2024-02-29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d25267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Spark\\\\spark-3.5.1-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fde3c2",
   "metadata": {},
   "source": [
    "# Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49368b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb2217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Titanic Data\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b05ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-O8EUHK6:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Titanic Data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a0bdaa3d10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1cdf03",
   "metadata": {},
   "source": [
    "# Reading Data\n",
    "\n",
    "â€¢ Dataset (Titanic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e2ee690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates DataFrame 'df' by reading a CSV file with Spark, including column names and specifying the file path.\n",
    "df = (spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"./Dataset/titanic/train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7f0a6508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7e833df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects and type transforms columns from DataFrame for analysis.\n",
    "dataset = df.select(col(\"Survived\").cast(\"float\"),\n",
    "                    col(\"Pclass\").cast(\"float\"),\n",
    "                    col(\"Sex\"),\n",
    "                    col(\"Age\").cast(\"float\"),\n",
    "                    col(\"Fare\").cast(\"float\"),\n",
    "                    col(\"Embarked\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "786d653c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-------+--------+\n",
      "|Survived|Pclass|   Sex| Age|   Fare|Embarked|\n",
      "+--------+------+------+----+-------+--------+\n",
      "|     0.0|   3.0|  male|22.0|   7.25|       S|\n",
      "|     1.0|   1.0|female|38.0|71.2833|       C|\n",
      "|     1.0|   3.0|female|26.0|  7.925|       S|\n",
      "+--------+------+------+----+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a77e22",
   "metadata": {},
   "source": [
    "# Importing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a61a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# StringIndexer is similar to labelencoder which gives a label to each category\n",
    "# OneHotEncoder created onehot encoding vector\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# VectorAssembler is used to create vector from the features. Modeling takes vector as an input \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# DecisionTreeClassifier is used for classiication problems\n",
    "from pyspark.ml.classification import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b476d7",
   "metadata": {},
   "source": [
    "# Using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "225a18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline from PySpark ML\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cfd683a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 562\n",
      "Number of test samples: 150\n"
     ]
    }
   ],
   "source": [
    "# Evaluator for multiclass classification with PySpark's ML library, set to Split data into training and test sets, remove NA values.\n",
    "(train_df, test_df) = df.na.drop().randomSplit([0.8, 0.2], 11)\n",
    "print(\"Number of train samples: \" + str(train_df.count()))\n",
    "print(\"Number of test samples: \" + str(test_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bd988a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creates index transformers for gender and port of disembarkation, then collects all features including these transformed, \n",
    "to finally model with a random forest classifier.\n",
    "'''\n",
    "sex_indexer = StringIndexer(inputCol=\"Sex\", outputCol=\"Gender\")\n",
    "embarked_indexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"Boarded\")\n",
    "\n",
    "# Assemble all the features with VectorAssembler\n",
    "input_cols = ['Pclass', 'Age', 'Fare', 'Gender', 'Boarded']\n",
    "output_col = \"feature\"\n",
    "vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=output_col)\n",
    "\n",
    "# Modeling using RandomForestClassifier\n",
    "dt_model = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71cc8efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+----+-------+--------+------+-------+-----------------------------------+--------------------------------------+---------------------------------------+----------+\n",
      "|Survived|Pclass|Sex |Age |Fare   |Embarked|Gender|Boarded|feature                            |rawPrediction                         |probability                            |prediction|\n",
      "+--------+------+----+----+-------+--------+------+-------+-----------------------------------+--------------------------------------+---------------------------------------+----------+\n",
      "|0.0     |1.0   |male|19.0|263.0  |S       |0.0   |0.0    |[1.0,19.0,263.0,0.0,0.0]           |[11.485763833304809,8.514236166695195]|[0.5742881916652404,0.4257118083347597]|0.0       |\n",
      "|0.0     |1.0   |male|21.0|77.2875|S       |0.0   |0.0    |[1.0,21.0,77.2874984741211,0.0,0.0]|[11.450676114006564,8.549323885993438]|[0.5725338057003282,0.4274661942996719]|0.0       |\n",
      "|0.0     |1.0   |male|28.0|82.1708|C       |0.0   |1.0    |[1.0,28.0,82.1707992553711,0.0,1.0]|[9.520066728477268,10.47993327152273] |[0.4760033364238634,0.5239966635761365]|1.0       |\n",
      "|0.0     |1.0   |male|29.0|30.0   |S       |0.0   |0.0    |[1.0,29.0,30.0,0.0,0.0]            |[11.12372621458774,8.876273785412263] |[0.556186310729387,0.44381368927061315]|0.0       |\n",
      "|0.0     |1.0   |male|29.0|66.6   |S       |0.0   |0.0    |[1.0,29.0,66.5999984741211,0.0,0.0]|[9.02053435137052,10.97946564862948]  |[0.45102671756852597,0.548973282431474]|1.0       |\n",
      "+--------+------+----+----+-------+--------+------+-------+-----------------------------------+--------------------------------------+---------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Configures a pipeline with preprocessing steps and model fitting, fits the model with training data, \n",
    "then makes predictions on test data and displays the first five results without truncation.\n",
    "'''\n",
    "# Setup the pipeline\n",
    "pipeline = Pipeline(stages=[sex_indexer, embarked_indexer, vector_assembler, dt_model])\n",
    "\n",
    "# Fit the Pipeline model\n",
    "final_pipeline_model = pipeline.fit(train_df)\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = final_pipeline_model.transform(test_df)\n",
    "\n",
    "test_predictions.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "150b4efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+---------------------------------------+--------+----------+\n",
      "|feature                            |probability                            |Survived|prediction|\n",
      "+-----------------------------------+---------------------------------------+--------+----------+\n",
      "|[1.0,19.0,263.0,0.0,0.0]           |[0.5742881916652404,0.4257118083347597]|0.0     |0.0       |\n",
      "|[1.0,21.0,77.2874984741211,0.0,0.0]|[0.5725338057003282,0.4274661942996719]|0.0     |0.0       |\n",
      "|[1.0,28.0,82.1707992553711,0.0,1.0]|[0.4760033364238634,0.5239966635761365]|0.0     |1.0       |\n",
      "+-----------------------------------+---------------------------------------+--------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting specific columns from the predictions shows the first three rows.\n",
    "prediction_display_format = test_predictions[\n",
    "    [\"feature\", \"probability\", \"Survived\", \"prediction\"]]\n",
    "\n",
    "prediction_display_format.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6e3aa",
   "metadata": {},
   "source": [
    "# Computing the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a01eaccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.78\n"
     ]
    }
   ],
   "source": [
    "# Imports evaluators, creates evaluators, calculates accuracy, prints training accuracy.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Survived', \n",
    "                                              predictionCol='prediction', \n",
    "                                              metricName='accuracy')\n",
    "\n",
    "accuracy = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"Training Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d3e12",
   "metadata": {},
   "source": [
    "# Checking Spark jobs\n",
    "\n",
    "After all you can check Spark Jobs on your local machine and manage them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821fc64",
   "metadata": {},
   "source": [
    "![](images\\5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ab62e",
   "metadata": {},
   "source": [
    "![](images\\6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b7d9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
